{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "if not TAVILY_API_KEY:\n",
    "    TAVILY_API_KEY = getpass.getpass(\"Tavily API key:\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LangChain_API_Key']=os.getenv('LangChain_API_Key')\n",
    "os.environ['LangChain_Project']=os.getenv('LangChain_Project')\n",
    "os.environ['Langchain_Tracing'] ='true'\n",
    "groq_api_key = os.getenv('Groq_API_Key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "urls =[\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs =[WebBaseLoader(web_path=url).load() for url in urls]\n",
    "docs_items =[item for sublist in docs for item in sublist]\n",
    "\n",
    "text_spliter =RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250,chunk_overlap=0)\n",
    "doc_split =text_spliter.split_documents(docs_items)\n",
    "\n",
    "# Add vectordb\n",
    "vectorestore =Chroma.from_documents(\n",
    "    documents=doc_split,\n",
    "    collection_name='rag_chroma',\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text')\n",
    ")\n",
    "\n",
    "retriver =vectorestore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# retrival grader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel,Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class GradDocument(BaseModel):\n",
    "    binary_score:str =Field(\n",
    "        description='Documents are relevant to the question are ,yes or no'\n",
    "    )\n",
    "\n",
    "llm =ChatGroq(\n",
    "    groq_api_key=groq_api_key ,\n",
    "    model ='qwen-2.5-32b',\n",
    "    temperature=0\n",
    "    )\n",
    "\n",
    "structured_llm_grader =llm.with_structured_output(GradDocument)\n",
    "\n",
    "# prompt \n",
    "system ='''you are grader assesing relevance of of retrived document to user question\\n\n",
    "if the document contains keywords or semantic meaning related to the question grad it is relevant\\n\n",
    "Give a binary scroe 'yes' or 'no' score to indicate whether the document is relevant to the question.'''\n",
    "\n",
    "grad_prompt =ChatPromptTemplate.from_messages([\n",
    "    ('system',system),\n",
    "    ('human',\"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "])\n",
    "\n",
    "retrival_grader_chain=(\n",
    "    grad_prompt\n",
    "    |structured_llm_grader\n",
    ")\n",
    "\n",
    "# invoke chain \n",
    "question ='agent memory'\n",
    "docs =retriver.get_relevant_documents(question)\n",
    "doc_txt =docs[1].page_content\n",
    "print(retrival_grader_chain.invoke({'question':question,'document':doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent memory includes both short-term and long-term memory. Short-term memory involves in-context learning, while long-term memory allows the agent to retain and recall information over extended periods using an external vector store for fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt =hub.pull('rlm/rag-prompt')\n",
    "\n",
    "# post processing \n",
    "def formate_documnets(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# chain \n",
    "rag_chain =(\n",
    "    prompt|llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "# run \n",
    "print(rag_chain.invoke({'question':question,'context':docs}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is agent memory and how does it work?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "print(question_rewriter.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Graph for CRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Graph State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List \n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    documents:List[str]\n",
    "    generation:str\n",
    "    web_search:str\n",
    "    question:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrive(state):\n",
    "    print('------Retriver-----')\n",
    "    question=state['question']\n",
    "    docs =retriver.get_relevant_documents(question)\n",
    "    return {'documents':docs,'question':question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print('----------generate------------')\n",
    "    question= state['question']\n",
    "    documents =state['documents']\n",
    "    generation =rag_chain.invoke({'context':documents,'question':question})\n",
    "\n",
    "    return {'documents':documents,'question':question,'generation':generation}\n",
    "\n",
    "def document_grader(state):\n",
    "    print('--check document relevant to the question--')\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrival_grader_chain.invoke({'question': question, 'document': d.page_content})\n",
    "        if score.binary_score.lower() == 'yes':\n",
    "            print('---document is relevant')\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print('--document is not relevant--')\n",
    "\n",
    "    # If none are relevant, set web_search\n",
    "    web_search = 'Yes' if len(filtered_docs) < len(documents) else 'No'\n",
    "\n",
    "    return {\n",
    "        'documents': filtered_docs,\n",
    "        'question': question,\n",
    "        'web_search': web_search\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "\n",
    "    print('----Transform Query---')\n",
    "    question =state['question']\n",
    "    documents =state['documents']\n",
    "\n",
    "    better_question =question_rewriter.invoke({'question':question})\n",
    "    return {'question':better_question,'documents':documents}\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# First, modify the web_search function to properly handle the search results\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Perform web search using Tavily API and add results to documents.\n",
    "    \"\"\"\n",
    "    print('---PERFORMING WEB SEARCH---')\n",
    "    \n",
    "    # Validate 'question'\n",
    "    question = state.get('question', '').strip()\n",
    "\n",
    "    if not question:\n",
    "        print(\"--- ERROR: Question is empty or missing ---\")\n",
    "        return {'documents': state.get('documents', []), 'question': question}\n",
    "\n",
    "    documents = state.get('documents', [])\n",
    "\n",
    "\n",
    "    # Get search results from Tavily\n",
    "    try:\n",
    "        search_results = web_search_tool.invoke(question)\n",
    "        # print(\"Search Results:\", search_results)\n",
    "\n",
    "        # Convert search results to Document objects\n",
    "        new_documents = []\n",
    "        for result in search_results:\n",
    "            if isinstance(result, dict) and 'content' in result:\n",
    "                web_result = Document(\n",
    "                    page_content=result['content'],\n",
    "                    metadata={'source': 'web_search'}\n",
    "                )\n",
    "                new_documents.append(web_result)\n",
    "\n",
    "        # Combine existing documents with new web search results\n",
    "        all_documents = documents + new_documents\n",
    "        \n",
    "        print(f'---FOUND {len(new_documents)} WEB SEARCH RESULTS---')\n",
    "        return {'documents': all_documents, 'question': question}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'---WEB SEARCH ERROR: {str(e)}---')\n",
    "        return {'documents': documents, 'question': question}\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END,StateGraph,START\n",
    "\n",
    "workflow =StateGraph(GraphState)\n",
    "\n",
    "# Define Nodes\n",
    "workflow.add_node('retrive',retrive)\n",
    "workflow.add_node('generate',generate)\n",
    "workflow.add_node('WebSearch_node',web_search)\n",
    "workflow.add_node('transform_query',transform_query)\n",
    "workflow.add_node('document_grader',document_grader)\n",
    "\n",
    "# build Graph\n",
    "workflow.add_edge(START,'retrive')\n",
    "workflow.add_edge('retrive','document_grader')\n",
    "workflow.add_conditional_edges('document_grader',\n",
    "decide_to_generate,\n",
    "{\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"transform_query\", \"WebSearch_node\")\n",
    "workflow.add_edge(\"WebSearch_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "app =workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Retriver-----\n",
      "'Node:retrive'\n",
      "--check document relevant to the question--\n",
      "--document is not relevant--\n",
      "--document is not relevant--\n",
      "---document is relevant\n",
      "---document is relevant\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "'Node:document_grader'\n",
      "----Transform Query---\n",
      "'Node:transform_query'\n",
      "---PERFORMING WEB SEARCH---\n",
      "---FOUND 5 WEB SEARCH RESULTS---\n",
      "'Node:WebSearch_node'\n",
      "----------generate------------\n",
      "'Node:generate'\n",
      "('AI systems, particularly those powered by large language models, utilize '\n",
      " 'short-term and long-term memory. Short-term memory involves in-context '\n",
      " 'learning, while long-term memory allows the retention and recall of '\n",
      " 'information over extended periods, often through external vector stores.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Run\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key,value in output.items():\n",
    "        pprint(f'Node:{key}')\n",
    "\n",
    "pprint(value['generation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Retriver-----\n",
      "\"Node 'retrive':\"\n",
      "'\\n---\\n'\n",
      "--check document relevant to the question--\n",
      "--document is not relevant--\n",
      "--document is not relevant--\n",
      "--document is not relevant--\n",
      "--document is not relevant--\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
      "\"Node 'document_grader':\"\n",
      "'\\n---\\n'\n",
      "----Transform Query---\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "---PERFORMING WEB SEARCH---\n",
      "---FOUND 5 WEB SEARCH RESULTS---\n",
      "\"Node 'WebSearch_node':\"\n",
      "'\\n---\\n'\n",
      "----------generate------------\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The AlphaCodium paper describes a test-based, multi-stage, and code-oriented '\n",
      " \"iterative flow for improving LLMs' performance on code problems. It utilizes \"\n",
      " 'a dataset called CodeContests, which includes competitive programming '\n",
      " 'problems and employs a unique methodology involving fine-tuning, clustering, '\n",
      " 'and iterative testing.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
